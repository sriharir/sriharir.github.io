---
layout: post
title:  "Linear Regression"
summary: Machine Learning Post 1
category: "tech"
---
This post is the first one in a series encapsulating concepts in the field of machine learning, trailing Andrew Ng's online course. As opposed to Ng's course which uses Octave, this series will use Python to explain concepts and solve some assignments. This lesson deals with linear regression, an algorithm used for prediction of an unknown  example after learning from known values.

Consider [this](https://www.hackerrank.com/challenges/predicting-house-prices) problem from Hackerrank.

Here, Charlie wants to predict the prices of houses based on various factors which he has already quantified in a value between 0 and 1, wiht the help of existing data. The problem can be solved using supervised learning in the form of linear regression.

Supervised learning involves the machine learning from previous examples, and using the "knowledge" to find answer to new similar questions. Charlie has already collected information to teach the computer, and we can use that as the "training set".

Consider this training set that uses one features and 7 examples to teach the machine. The first value is the feature of the house (say, proximity to supermarket), and the second value is the result, i.e. the price per square foot for the house:

{% highlight python %}
0.18 109.85
1.0 155.72
0.92 137.66
0.07 76.17
0.85 139.75
0.99 162.6
0.87 151.77
{% endhighlight %}

The following are the notations used : 

$$
\begin{align*}
\\m : number\,of\,training\,examples 
\\x^{(i)} : i^{th}\,training\,example's\,feature\,value
\\y^{(i)} : i^{th}\,training\,example's\,result\,value
\\h^{(i)}(x) : i^{th}\,example's\,predicted\,value
\end{align*}
$$

After learning, Charlie enters the the value of the feature, i.e. proximity of the new house, and asks the machine to predict the price. Let's look at how we can teach the machine to do this task. First, let us see how the above values look on a graph.

The following code plots the above values manually entered using the matplotlib module. Make sure you have 'numpy' and 'matplotlib' packages installed with your Python. 

{% highlight python %}
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cbook as cbook

X=np.array([0.18, 1.0, 0.92,0.07,0.85,0.99,0.87])
y=np.array([109.85, 155.72, 137.66, 76.17,139.75, 162.6, 151.77])
plt.scatter(X,y,marker='x',color='r',s=100)
plt.xlabel('Proximity to Market', fontsize=15)
plt.ylabel('Price per square foot', fontsize=15)
plt.show()
{% endhighlight %}

The code returns the following plot:

![Image 1]({{ site.url }}/assets/figure_1.png)

Now our goal is to generate an approximate function that fits the above given value, so that we can predict price for different values of proximity. We are just gonna go forward and assume, that the relation between price in our approximate function (also called the hypotheses) ($$h^{(i)}(x)$$) and the feature ($$x^i$$) (proximity to supermarket) is linear. Assume :

$$
\begin{align*}
h^{(i)}(x)=\theta_0 +\theta_1*x^i
\end{align*}
$$

Now, our goal would be to find $$ \theta_0 $$ and $$ \theta_1 $$ such that the above function fits the scatter plot we earlier saw as well as it could. Once we get such $$ \theta_0 $$ and $$ \theta_1 $$ we can use it to find price for any value of proximity to supermarket.

From the above plot, we can reasonably assume what could be good values of $$ \theta_0 $$ and $$ \theta_1 $$. Let $$ \theta_0 = 80 $$ and $$ \theta_1 = 80 $$. Now we plot this function over the scatter plot of the data we have using the code below :

 {% highlight python %}
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.cbook as cbook

X=np.array([0.18, 1.0, 0.92,0.07,0.85,0.99,0.87])
y=np.array([109.85, 155.72, 137.66, 76.17,139.75, 162.6, 151.77])
plt.scatter(X,y,marker='x',color='r',s=100)
plt.xlabel('Proximity to Market', fontsize=15)
plt.ylabel('Price per square foot', fontsize=15)
plt.plot([0, 1], [90, 160])
plt.show()
{% endhighlight %}
  

We get the following plot :

![Image 2]({{ site.url }}/assets/linreg2.png)

This does not look like a bad approximation. But we can be sure that the machine can do better than this, by bringing down the error to minimum possible. The error can be calculated using what is termed the "error function". One common error function that is widely used is:

$$
\begin{align*}
J(\theta_0,\theta_1)=\frac{1}{2m} 
 \left(\sum_{i=1}^m (h^{(i)}(x) - y^{(i)})^2\right)
\end{align*}
$$

This is essentially the average of squares of difference between the predicted value from our linear approximation and the real value in the data set. Now, this can be used as a measure of how accurate our linear approximation is. Very intuitively, a good approximation would minimize the above error function. Let's look at an example to just understand cost function, and the article will come back to Charlie's example later.

Consider this simpler training set with m=4.

 {% highlight python %}
0 0
1 1
2 2
3 3
{% endhighlight %}

Here's the plot :

![Image 3]({{ site.url }}/assets/linreg3.png)

Let's assume that all the approximation are of the form $$h^{(i)}(x)=\theta_0x^i$$ for the sake of simplicity. Obviously, the best fit here would be $$h^{(i)}(x)=x^i$$:



![Image 4]({{ site.url }}/assets/linreg4.png)


For the hypotheses $$h^{(i)}(x)=\theta_0x^i$$, error function is given by:

$$
\begin{align*}
J(\theta_0,\theta_1)=\frac{1}{2m} 
 \left(\sum_{i=1}^m (\theta_0*x^{(i)} - y^{(i)})^2\right)
\end{align*}
$$

If $$\theta_0=1$$, by substitution of values from the training data table above, $$J(\theta_0)=0$$. Consider another bad hypotheses, with $$\theta_0=2$$.

![Image 5]({{ site.url }}/assets/linreg5.png)

By definition of cost function, the cost function should be the sum of squares of the lengths of green lines in the following plot divide by $$2m$$:

![Image 6]({{ site.url }}/assets/linreg6.png)

$$
\begin{align*}
J(\theta_0=2)=\frac{1}{2*4}
\left((0-0)^2+(2-1)^2+(4-2)^2+(6-3)^2\right)=\frac{7}{4}
\end{align*}
$$
Figured?

The rest of the article deals with means to minimize the cost function, or to rephrase, the procedure to find the best $$\theta_0$$ and $$\theta_1$$ (in the above example, only $$\theta_0$$). With some background in calculus, it is easy to figure that, if a function is increasing with the independent variable, then it's derivative is positive, and when it's decreasing, it's derivative is negative. So for this example, let's plot variation of $$J(\theta_0)$$ with $$\theta_0$$.

![Image 7]({{ site.url }}/assets/linreg7.png)

At any $$\theta_0$$ our aim is to change it so that, $$\theta_0$$ gives the minimum value of $$J(\theta_0)$$. The value occurs at $$\frac{dJ(\theta_0)}{d\theta_0}=0$$. So, if $$\frac{dJ(\theta_0)}{d\theta_0}>0$$ it means, the function is increasing with $$\theta_0$$, and we need to reduce $$\theta_0$$. So, if $$\frac{dJ(\theta_0)}{d\theta_0}<0$$ it means, the function is decreasing with $$\theta_0$$, and we need to increase $$\theta_0$$ until we reach the minima. So this can be expressed as:

$$
\begin{align*}
\theta_0:=\theta_0-\alpha*\frac{dJ(\theta_0)}{d\theta_0}
\end{align*}
$$

Here, $$\alpha$$ controls the rate of change. If the hypotheses was of the form as in Charlie's example, i.e. $$h^{(i)}(x)=\theta_0 +\theta_1*x^i$$, then the change in $$ \theta_0 $$ and $$ \theta_1 $$ can be expressed as:

$$
\begin{align*}
\theta_i:=\theta_i-\alpha*\frac{\partial J(\theta_0, \theta_1)}{\partial\theta_i}\;\;\;where i=0,1
\end{align*}
$$

To be continued...