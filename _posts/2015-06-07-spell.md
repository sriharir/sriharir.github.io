---
layout: post
title:  "Spelling Correction"
summary: NLP Post 1
category: "tech"
---
This article talks about my implementation of a spelling corrector. I personally think it is slightly inefficient and performs couple of redundant steps, and could be made to work more efficiently by using appropriate algorithms and data structures. For now, this articles just focuses on the NLP aspect of the application only.

The algorithm used is known as "noisy channel model" for spelling correction. The idea is simple. We have a word that is wrongly spelled, and a noisy medium, through which the original word "traveled" before it turned into the misspelled word it is. For example, the word "mispeled" could be the result of some correctly spelled word traveling through the noisy medium. Now our goal is to find the original correctly spelled word.

Let the misspelled word be $$x$$. Consider a word $$w$$. What is the probability that $$x$$ was created by $$w$$ traveling through the noisy medium? What is,

$$p(w|x)$$

$$p(w|x)=\frac{p(x|w).p(m)}{p(x)}$$

To find out the correct spelling, among all candidates for $$w$$, we choose that w that gives maximum value for p(w|x)

$$w=arg max_w(\frac{p(x|w).p(w)}{p(x)})$$

Since, for all w, $$p(x)$$ is same, 

$$w=arg max_w(p(x|w).p(w))$$

Now, on finding values of $$p(w)$$ and $$p(x\vert w)$$. $$p(w)$$ is simply the probability of $$w$$ occurring. This could be found out by counting $$w$$ in a corpus and dividing it with total number of words in the corpus. For better results, we could look into the sentence in which the spelling error under consideration happened, and use bigram or trigram probabilities by asking questions like, what is the probability that $$w$$ comes after $$amazing$$, or such. But for this simple code, I've only used unigram probability, that is $$p(w)$$ is simply the probability of word $$w$$ occurring

Now on $$p(x\vert w)$$.
It translates to, "What is the probability of the given misspelling, $$x$$, if the original word was $$w$$?". Here's where the "noisy model" comes into relevance. Here we train the model from existing data on common misspellings, to figure out what kind of mistakes happen the most. Like, we count how many times, 'ie' is replaced by 'ei' or 'a' is replaced with 'e' and such. When we have a dictionary of all kinds of errors, we can see how frequent each kind of error is, and use that to predict that probability of spelling $$w$$ wrongly as $$x$$. 

Now, there are four kinds of errors we consider. There are, 1) insert an extra letter, 2) delete a letter, 3) replace one letter with another, 4) transpose two letters. So if we want to find $$p(x\vert w)$$, we find out the errors in $$w$$ that made it $$x$$, look up in the "noisy model" dictionary to find out the probability that these errors occur in the general corpus, and multiply those to get the probability $$w$$ was misspelled as $$x$$.

Now, here's the part of code that trains the noisy channel model. It is trained using the errors listed [here](http://norvig.com/ngrams/spell-errors.txt):

{% highlight python %}
from collections import defaultdict
import re
import sys

path='/home/rsrihari/Documents/codes/spell_corrector/'
print '\nWelcome to Srihari\'s spell correction portal. Enter your sentence when prompted, and press enter to get correct spellings'

#dictionaries to build the noisy channel model.
#Stores count of each error
insd=defaultdict(int)
detd=defaultdict(int)
subd=defaultdict(int)
trsd=defaultdict(int)


#returns the minimum of three numbers, and also a string
#denoting the kind of edit made
def min(a, b,c=1000):
    if a<b:
        if a<c:
            return (a,'ins')
        else:
            return (c,'sub')
    else:
        if b<c:
            return (b,'det')
        else:
            return (c,'sub')

#simply returns min of two numbers
def minof2(a,b):
    if a>b:
        return b
    else:
        return a

#finds out the edit distance and edit steps 
#between the correct and wrong spelling.
#used to find out the edits made to wrong spelling to
#make it correct, and store them in noisy channel model
#dictionaries for model construction

def editdist(str1, str2):
    m=len(str1)+1
    n=len(str2)+1
    table=[]
    for i in range(m):
        table.append([])
        for j in range(n):
            table[i].append(0)
    for i in range(m):
        table[i][0]=(i,'ins')
    for j in range(n):
        table[0][j]=(j,'det')
    for i in range(1,m):
        for j in range(1,n):
            if(str1[i-1]==str2[j-1]):
                cost=0
            else:
                cost=1
            ins=table[i-1][j][0]+1
            det=table[i][j-1][0]+1
            sub=table[i-1][j-1][0]+cost
            table[i][j]=min(ins,det,sub)
            if(i>1 and j>1 and str1[i-1]==str2[j-2] and str1[i-2]==str2[j-1]):
                table[i][j]=(minof2(table[i][j][0],table[i-2][j-2][0]+cost),'trs')
    i=m-1
    j=n-1
    code=0
    while(i!=0 or j!=0):
        if table[i][j][1]=='det':
            if(j>1):
                #print 'del - ', str2[j-1],'after',str2[j-2]
                detd[str2[j-2]+str2[j-1]]+=1
            if j==1:
                #print 'del -', str2[j-1],'at the beginning'
                detd[str2[j-1]]+=1
            j=j-1
        if table[i][j][1]=='ins':
            if i>1:
                #print 'ins - ', str1[i-1],'after',str1[i-2]
                insd[str1[i-2]+str1[i-1]]+=1
            if i==1:
                #print 'ins-', str1[i-1], 'at the beginning'
                insd[str1[i-1]]+=1
            i=i-1
        if table[i][j][1]=='sub':
            if(str2[j-1]!=str1[i-1]):
                #print 'sub - ', str2[j-1],'with',str1[i-1]
                subd[str2[j-1]+str1[i-1]]+=1
            i=i-1
            j=j-1
        if table[i][j][1]=='trs':
            if(str2[j-1]!=str2[j-2]):
                #print 'transpose - ', str2[j-1],'with',str2[j-2]
                trsd[str2[j-1]+str2[j-2]]+=1
            i=i-2
            j=j-2
    return table[m-1][n-1][0]

f=open(path+'mispled2.txt')
error=f.readlines()

errorsn=0  #to count total number of errors caught in learning corpus 
for i in error:
    words=re.findall('\w+\s?\w+',i)
    for k in range(1, len(words)):
        errorsn+=editdist(words[0],words[k]) #this loop trains the noisy channel model
        
print errorsn
{% endhighlight %}

Next part is, using this model. To find out the word that gives maximum probability of being misspelled as $$x$$, we might need to go over all the words. But luckily, most errors are usually withing one or two edits of the original words. So in our code, we use functions to generate all existing words that are within one or two edits of the spelling mistake, and choose the most probable among them as the correct spelling. We find out these words using function 'edits1','edits2', and 'known'. The existing words are chosen by referring to a dictionary of words from another general corpus (not the misspelled word corpus), which is also used to find the probability of any word.

Here's that part of the code. Note we also added a function editdistg for finding edits made to the wrong word to convert it into the word we compare it with while finding probability $$p(x\vert w)$$, because we don't want to add the edits to the model dictionary.

The words are validated using it's presence in [THIS](norvig.com/big.txt) file. The same file is used for counting frequencies, and hence probabilities of each word.

{% highlight python %}
#editdistg - same as editdist, only now it is used on
#test data without adding anything to dictionary 
#but returning the kind of edits made in a list
def editdistg(str1, str2):
    editmade=[]
    m=len(str1)+1
    n=len(str2)+1
    table=[]
    for i in range(m):
        table.append([])
        for j in range(n):
            table[i].append(0)
    for i in range(m):
        table[i][0]=(i,'ins')
    for j in range(n):
        table[0][j]=(j,'det')
    for i in range(1,m):
        for j in range(1,n):
            if(str1[i-1]==str2[j-1]):
                cost=0
            else:
                cost=1
            ins=table[i-1][j][0]+1
            det=table[i][j-1][0]+1
            sub=table[i-1][j-1][0]+cost
            table[i][j]=min(ins,det,sub)
            if(i>1 and j>1 and str1[i-1]==str2[j-2] and str1[i-2]==str2[j-1]):
                table[i][j]=(minof2(table[i][j][0],table[i-2][j-2][0]+cost),'trs')
    i=m-1
    j=n-1
    code=0
    while(i!=0 or j!=0):
        if table[i][j][1]=='det':
            if(j>1):
                #print 'del - ', str2[j-1],'after',str2[j-2]
                detdg[str2[j-2]+str2[j-1]]+=1
                editmade.append(('det',str2[j-2]+str2[j-1]))
                
            if j==1:
                #print 'del -', str2[j-1],'at the beginning'
                detdg[str2[j-1]]+=1
                editmade.append(('det',str2[j-1]))

            j=j-1
        if table[i][j][1]=='ins':
            if i>1:
                #print 'ins - ', str1[i-1],'after',str1[i-2]
                editmade.append(('ins',str1[i-2]+str1[i-1]))
            if i==1:
                #print 'ins-', str1[i-1], 'at the beginning'
                editmade.append(('ins',str1[i-1]))
            i=i-1
        if table[i][j][1]=='sub':
            if(str2[j-1]!=str1[i-1]):
                #print 'sub - ', str2[j-1],'with',str1[i-1]
                editmade.append(('sub',str2[j-1]+str1[i-1]))
            i=i-1
            j=j-1
        if table[i][j][1]=='trs':
            if(str2[j-1]!=str2[j-2]):
                #print 'transpose - ', str2[j-1],'with',str2[j-2]
                editmade.append(('trs',str2[j-1]+str2[j-2]))
            i=i-2
            j=j-2
    return editmade

def words(text): return re.findall('[a-z]+', text.lower()) 

#returns a dictionary of word count in corpus
def train(features):
    model = defaultdict(lambda: 1)
    for f in features:
        model[f] += 1
    return model

NWORDS = train(words(file(path+'big.txt').read())) #dictionary of word counts in corpus


alphabet='abcdefghijklmnopqrstuvwxyz'

#this function returns set of all words (existing and non existing)
#that could be made using one edit to the argument word
def edits1(word):
   splits     = [(word[:i], word[i:]) for i in range(len(word) + 1)]
   deletes    = [a + b[1:] for a, b in splits if b]
   transposes = [a + b[1] + b[0] + b[2:] for a, b in splits if len(b)>1]
   replaces   = [a + c + b[1:] for a, b in splits for c in alphabet if b]
   inserts    = [a + c + b     for a, b in splits for c in alphabet]
   return set(deletes + transposes + replaces + inserts)

#this function uses edits1 to return set of all words (only existing)
#that could be made using two edits to argument word
def known_edits2(word):
    return set(e2 for e1 in edits1(word) for e2 in edits1(e1) if e2 in NWORDS)

#returns set of exisiting words from set of passed words
def known(words): return set(w for w in words if w in NWORDS)
{% endhighlight %}

The last part is taking input from user and using the function in the above part to generate similar words, and run it with the noisy model to find out the most probable original word.

{% highlight python %}
sentence='start'
while(sentence[0]!='exit'):
    print '\nEnter new sentence : '
    sentence=raw_input().split()
    for wrongword in sentence:
        word=wrongword
        #the following part creates a list of similar words to compare
        #probabilities of p(x|w)*p(w)
        if len(word)>4:
            simiwords = known([word]).union(known(edits1(word))).union(known_edits2(word))
        elif len(word)>1:
            simiwords = known([word]).union(known(edits1(word)))
        else:
            simiwords=[word]
        maxprob=0
        #print simiwords
        for i in simiwords:
            multi=1 #set probability as 1
            count=NWORDS[i] #count of the current similar word
            #measure of p(w)
            editsmade=editdistg(i, wrongword)
            #this part calculate p(x|w) for current similar word
            #using the trained model
            for editis in editsmade:
                if editis[0]=='ins':
                    multi*=(insd[editis[1]]+1)*1.0/errorsn
                if editis[0]=='det':
                    multi*=(detd[editis[1]]+1)*1.0/errorsn
                if editis[0]=='sub':
                    multi*=(subd[editis[1]]+1)*1.0/errorsn
                if editis[0]=='trs':
                    multi*=(trsd[editis[1]]+1)*1.0/errorsn
            multi=multi*1000.0
            if editsmade==[]:
                multi=100
            curr_prob=(count)*(multi)
            if maxprob<curr_prob:
                maxprob=curr_prob #p(x|w)*p(w)
                word=i
        print word,
    
{% endhighlight %}

Tada, that's all. Oh, and here's a screenshot of results produced by this application.

![Shot]({{ site.url }}/assets/spell.jpg)


